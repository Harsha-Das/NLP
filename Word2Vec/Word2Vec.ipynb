{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps to Create Word2Vec:-\n",
    "\n",
    "    1. Tokenization of the sentences \n",
    "    2. Create Histograms\n",
    "    3. Take most frequent words\n",
    "    4. Create a matrix with all the unique words. It also represents the occurence of\n",
    "       relation between the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ',paragraph)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "text = text.lower()\n",
    "text = re.sub(r'\\d',' ',text)\n",
    "text = re.sub(r'\\s+',' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataset\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Word Vectors\n",
    "vector = model.wv['war']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00208731 -0.0045424   0.00347627  0.00269726  0.00231676  0.00292798\n",
      "  0.00425407 -0.00262867  0.00340922  0.00454316 -0.00325676 -0.00497139\n",
      "  0.00203205 -0.00041882  0.00130802  0.00494852  0.00459669 -0.0048016\n",
      "  0.00170122  0.00423666 -0.00454797 -0.00446174 -0.00236345  0.00192448\n",
      " -0.00048526 -0.00170907  0.00209021 -0.00102553 -0.00016415 -0.00416506\n",
      " -0.00113984  0.00149294  0.00409546  0.0031172  -0.00310674 -0.00285384\n",
      " -0.00359805 -0.00107371 -0.00285392 -0.0032553  -0.0003002   0.00046333\n",
      "  0.00317362  0.00040418 -0.00252809 -0.00192473 -0.00025982 -0.00493308\n",
      "  0.00108119  0.00027881  0.00448791  0.00468739  0.00292707  0.00336868\n",
      "  0.00089733 -0.00204442 -0.00251204  0.00387493 -0.00046721 -0.00159412\n",
      " -0.00181493 -0.00070058 -0.00153865  0.00260223  0.00303762 -0.00109784\n",
      " -0.00472096 -0.00321686  0.00404496  0.00234353 -0.00099087 -0.00182009\n",
      "  0.00123397  0.00459755  0.00088786  0.00189979  0.00485316  0.00238043\n",
      " -0.00180052  0.0017252  -0.00291957 -0.00222424 -0.00402003 -0.00426042\n",
      "  0.00202295  0.00174712 -0.00125794  0.00034368  0.00371725  0.00159459\n",
      " -0.00314204  0.00021768 -0.00420707  0.0004263   0.00202886  0.00478686\n",
      "  0.00481713  0.00278621  0.00251465 -0.00486002]\n"
     ]
    }
   ],
   "source": [
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most similar words\n",
    "similar = model.wv.most_similar('war')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('greeks', 0.24359583854675293), ('dutch', 0.24342966079711914), ('us', 0.24248605966567993), ('professor', 0.16892702877521515), ('developed', 0.13905346393585205), ('among', 0.13684053719043732), ('one', 0.1358042061328888), ('vikram', 0.12653326988220215), ('good', 0.12499403208494186), ('third', 0.11976833641529083)]\n"
     ]
    }
   ],
   "source": [
    "print(similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
